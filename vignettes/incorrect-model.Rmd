---
title: "Estimating the distribution of the M-estimator of a general loss function"
author: "Qian Zhao"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{resized-bootstrap-general-m-estimator}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 5, fig.width = 4, fig.align = "center")
library(ggplot2)
```


```{r load_package, include = F}
devtools::load_all("/Users/zq/Documents/GitHub/glmhd")
```


In this vignette, we consider the situation when we use a different loss function than the negative log-likelihood. For example, when we fit a logistic regression when the $Y |X$ is from a Probit model. Here, we assume that we know what the true model is (though this assumption may be infeasible in practice). 

We will discuss two ways to estimate the distribution of the M-estimator. First, suppose that the covariates are multivariate Gaussian (or sub-Gaussian), then we can apply the high-dimensional theory. Second, when the covariates are **not** Gaussian, we illustrate how to use the resized bootstrap method to estimate the distribution of the M-estimator. 

```{r, set_seed, echo = F}
set.seed(2) 
```

### A simulated example when the covariates are Gaussian

We first consider an example when the covariates are multivariate Gaussian. We set the number of obs. $n=1000$ and the number of variables $p = 200$ (the problem dimension is $\kappa = p/n = 0.2$) and set $X\sim N(0,\Sigma)$, where $\Sigma$ is a circulant matrix $\Sigma_{i,j} = 0.5^{\min(|i-j|, p-|i-j|)}$. We standardize each variable to have zero mean and variance equal to $1/n$. Half of the variables are sampled to be non-nulls and the effect size are $\pm 4$ (half of the non-nulls are equal to 4 and the other half are equal to $-4$). Next, we will sample $Y\,|\, X$ from a probit model, that is, $Y\in\{\pm 1\}$ where $\mathrm{P}(Y=1|X) = \Phi(X^\top \beta)$ where $\Phi$ is the normal cdf. Finally, we fit a logistic regression for each data $(X, Y)$ and compute the logistic MLE from each sample. The following code snippet illustrtes how to sample the model coefficients.

```{r, sample_gaussian_obs, echo = F }
sample_gaussian_obs <- function(n, beta, R){
  p <- length(beta) # number of variables
  # Sample X
  X <- matrix(rnorm(n * p, 0, 1), n, p) %*%  R / sqrt(n)
  # Sample Y from a probit model 
  Y <- rbinom(n, 1, pnorm(X %*% beta))
  list(X = X, Y = Y)
}

fit_logistic <- function(X, Y){
  # Logistic regression 
  fit <- glm(Y ~ X + 0, family = binomial, x = TRUE, y = TRUE)
  # Returns the fitted coefficients and their standard error estimates
  c(fit$coef, summary(fit)$coef[,2])
}
```


```{r model_param}
n <- 1000 
p <- 200 
kappa <- p/n

nu <- 8
rho <- 0.5
x <- rho^(c(0:(p/2), (p/2-1):1))
Sigma <- toeplitz(x)
R <- chol(Sigma)

ind <- sample(1:p, p,replace = F)
null <- ind[1:(p/2)]
nonnull <- ind[(p/2+1):p]
beta <- numeric(p)
beta[nonnull[1:(p/4)]] <- 4; beta[nonnull[(p/4 + 1):(p/2)]] <- -4
```

We repeat the simulation `100` times to compute the inflation and std.dev. of the M-estimators.

```{r}
one_sim_gaussian <- function(n, beta, R){
  obs <-  sample_gaussian_obs(n, beta, R)
  result <- fit_logistic(obs$X, obs$Y)
  result
}
result <- replicate(100, one_sim_gaussian(n, beta, R))
```

```{r, echo = F}
beta_hat <- result[1:p, ] # p by B MLE matrix
std_hat <- result[(p+1):(2*p), ] # p by B matrix of the estimated std.dev. 
j <- nonnull[1] # focus on one coordinate
betahatj <- rbind(beta_hat[j,], std_hat[j,]) 
```


We can compute the inflation and std.dev of a single nonnull variable The mean of the MLE is `r round(mean(betahatj[1,]), 2)`, i.e., it is inflated by a factor of `r round(mean(betahatj[1,])/4,  2)`. The empirical std.dev is `r round(sd(betahatj[1,]), 2)`. In comparison, the classical theory estimate of the std. dev. is on average `r round(mean(betahatj[2,]),2)` which is `r round((sd(betahatj[1,]) - mean(betahatj[2,])) / sd(betahatj[1,]) *100, 2)`\% smaller than the empirical std. dev. Although the MLE is unbiased in the classical setting where $p$ is fixed or $p/n\to 0$, the MLE is biased upward in magnitude when $p/n$ is not negligible.  

#### Estimated inflation and std.dev. using the High-simensional theory

To illustrate the high-dimensional theory, we first generate a new observation.

```{r}
obs <-  sample_gaussian_obs(n, beta, R)
fit <- glm(obs$Y~obs$X + 0, family = binomial, x = T, y = T)
```

We first illustrate how to estimate the signal strength parameter $\gamma$ using the Probe Frontier method. We start by finding a problem dimension $\kappa_s$ above which the two classes become separable.

```{r}
kappa_s <- probe_frontier(obs$X, 2*obs$Y - 1, B = 3, eps = 0.001, verbose = FALSE)
```

Next, we will estimate the signal strength $\gamma$ using the function `signal_strength`. Here, we specify that the true model is a probit model through the parameter $\rho'(t)$ which is the probability of $Y = 1 | X^\top \beta = t$, i.e. $\rho'(t) = \mathrm{P}(Y = 1 | X^\top \beta = t)$.  

```{r, solve_gamma}
rho_prime <- function(t) pnorm(t)
gamma_hat <- signal_strength(rho_prime = rho_prime, kappa_hat = kappa_s) 
```

Finally, we solve a system of three nonlinear equations to obtain the parameters $(\alpha_\star,\sigma_\star,\lambda_\star)$. Here, we will input three parameters: $\rho'$ as before;  $f'_1(t)$ which is the derivative of the loss function when $Y = 1$. For the logistic regression, $f'_1(t) = -1 / (1 + \exp(x))$; $f'_0(t)$ which is the derivative of the loss function when $Y = 0$. For the logistic regression, $f'_0(t) = 1 / (1 + \exp(-x))$.

```{r}
fprime_1 <- function(x) -1 / (1 + exp(x))
fprime_0 <- function(x) 1 / (1 + exp(-x))
params <- find_param(rho_prime = rho_prime, f_prime1 = fprime_1,
                     f_prime0 = fprime_0,
                     kappa = kappa,
                     gamma = gamma_hat$gamma_hat, 
                     beta0=0, intercept = F)

params[1] # estimated inflation using the HDT
tau <- 1/sqrt(diag(solve(Sigma)))
params[3]/sqrt(kappa)/tau[j] # estimated std.dev.
```

This yields estimated inflation and std.dev that equal to `r round(params[1], 2)` and `r round(params[3]/sqrt(kappa)/tau[j], 2)` respectively. 

### An example of non-Gaussian covariates

Suppose the covariates are not Gaussian, then the high-dimensional theory no longer applies, and yet we can still apply the resized bootstrap method. Note that to apply the resized bootstrap method in this case, we should simulate $Y|X$ from a Probit model instead of from a logistic model .

In this example, we simluate some of $X$ from a multivariate Gaussian distribution and the rest as *interactions* between variables.  

We set $n=1000$ and $p=200$, the first half of the covariates are $N(0,\Sigma)$ with the same circulant covariance matrix as before. The second half are interactions between variables. We standardize $X_j$ to have variance equal to $1/p$. 

```{r setting-nongaussian}
nInt <- p / 2 # number of interaction terms

# define covariance matrix
rho <- 0.5
x <- rho^(c(0:((p - nInt)/2), ((p - nInt)/2-1):1))
SigmaSmall <- toeplitz(x) 
R <- chol(SigmaSmall)

# sample interaction terms
indices <- matrix(sample(1:(p-nInt), 2000,replace = T), ncol = 2)
indices <- t(apply(indices, 1, sort))
indices <- unique(indices)[1:nInt, ]

# sample model coefficients
nonnull1 <- sample(1:p, 0.5 * p, replace = F)
beta1 <- numeric(p)
beta1[nonnull1] <- rnorm(length(nonnull1), 0, 1.5)
```


```{r, sample-nongaussian, echo = F }
# function to sample one obs 
sample_nongaussian_obs <- function(n, beta, R){
  X <- matrix(rnorm(n* (p-nInt), 0, 1), n, (p-nInt)) 
  X <- X %*% R
  XInt <- apply(indices, 1, function(t) X[,t[1]] * X[,t[2]])
  X <- cbind(X, XInt) / sqrt(p)
  
  # Sample Y from a probit model 
  Y <- rbinom(n, 1, pnorm(X %*% beta))
  list(X = X, Y = Y)
}

one_sim_nongaussian <- function(n, beta, R){
  obs <-  sample_nongaussian_obs(n, beta, R)
  result <- fit_logistic(obs$X, obs$Y)
  result
}
```


```{r mle-nongaussian, echo = F}
beta_hat <- replicate(100, one_sim_nongaussian(n, beta1, R))

mle <- beta_hat[1:p, ] # MLE
sdR <- beta_hat[-(1:p), ] # std.dev
```

```{r, echo = F}
j <- sort(beta1, decreasing = T, index.return = T)$ix[40]
```

We observe an inflation of `r round(mean(mle[j, ]) / beta1[j] ,2)` and std.dev. of `r round(sd(mle[j, ]),2)` in 100 repeated simulations. In comparison, the classical theory estimate of the std.dev. is `r round(mean(sdR[j,]),2)`. We plot the average MLE versus the true coefficients for every variable. The plot suggests that the inflation of the MLE are similar to each other and the overall inflation is about `r round(lm(rowMeans(mle)~beta1)$coef[2], 2)`. 


```{r echo = F}
ggplot() + 
  geom_point(aes(x = beta1, y =  rowMeans(mle))) + 
  geom_abline(slope = lm(rowMeans(mle)~beta1)$coef[2], intercept =  0 ) + 
  theme_bw() + 
  xlab("beta") + 
  ylab("Average MLE") + 
  theme(text = element_text(size = 15))
```

#### Using the high-dimensional theory

We apply the high-dimensional theory as before and compare the theoretical inflation and std.dev..

```{r one-nongaussian-obs}
obs <-  sample_nongaussian_obs(n, beta1, R)
fit <- glm(obs$Y~obs$X + 0, family = binomial, x = T, y = T)
```

```{r}
kappa_s <- probe_frontier(obs$X, 2*obs$Y - 1, B = 3, eps = 0.001, verbose = FALSE)
gamma_hat <- signal_strength(rho_prime = rho_prime, kappa_hat = kappa_s) 
params <- find_param(rho_prime = rho_prime, f_prime1 = fprime_1,
                       f_prime0 = fprime_0,
                       kappa = kappa,
                        beta0=0, intercept = F,
                       gamma = gamma_hat$gamma_hat, x_init=c(2.5, 1, 1.5))
```

```{r re-estimate-sigma}
# Estimate covariance matrix because X is not Gaussian
X <- sample_nongaussian_obs(10000, beta1, R)$X
Sigma <- t(X) %*% X / 10000
tau <- 1/sqrt(diag(solve(Sigma)))
```


The theoretical inflation using the HDT is `r round(params[1],2)`. The estimated std.dev. is `r round(params[3]/tau[j]/sqrt(p),2)`,  and the relative error of the theoretical estimate is `r round(abs(params[3]/tau[j]/sqrt(p) - sd(mle[j, ])) / sd(mle[j, ]), 3)* 100`\%. The relative error (averaged over all of the coordinates) is `r round(mean(abs(params[3]/tau[j]/sqrt(p) - apply(mle, 1, sd))/apply(mle, 1, sd)), 3) * 100`\%. 

### The resized bootstrap method

Finally, we illustrate how to use the resized bootstrap method to estimate the distribution of the M-estimator. Here, we need to define how to simulate the new observation $Y|X$. By default, the function `glm_boot` uses the family and link defined in the `glm` fit. Because $Y|X$ is from a Probit model, we define a new function to simulate $Y | X$.

```{r new-dim-fun}
simulate_fun_probit <- function(X, beta){
  t <- X %*% beta
  probs <- pnorm(t)
  rbinom(length(t), 1, probs)
}
```

Then, we input `simulate_fun_probit` as a parameter in `glm_boot`.

```{r, resized_boot_nongaussian, fid.height = 3, fig.width = 4, fig.align='center'}
boot_fit <- glm_boot(fit, simulate_fun = simulate_fun_probit, s_interval = 0.02, b_var = 5, b_boot = 100)
```

The estimated inflation and std.dev. are `r round(boot_fit$alpha, 2)` and `r round(boot_fit$sd[j],2)`, which has relative error `r round((boot_fit$sd[j] - sd(mle[j, ]))/sd(mle[j, ]),3) * 100`\%. The relative error (averaged over all of the coordinates) is `r round(mean(abs(boot_fit$sd - apply(mle, 1, sd))/apply(mle, 1, sd)), 3) * 100`\%. 








